{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Source: \n",
        "\n",
        "*   https://pypi.org/project/gpt-2-simple/#description\n",
        "*   https://medium.com/@stasinopoulos.dimitrios/a-beginners-guide-to-training-and-generating-text-using-gpt2-c2f2e1fbd10a\n",
        "*   https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce#scrollTo=VHdTL8NDbAh3\n",
        "*  https://github.com/ak9250/gpt-2-colab\n",
        "*  https://www.aiweirdness.com/d-and-d-character-bios-now-making-19-03-15/\n",
        "*  https://minimaxir.com/2019/09/howto-gpt2/\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rgNM-NcAZ9aT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/zawemi/GS2DIT/blob/main/Class%203/gpt_2_shakespeare.ipynb#scrollTo=4tIUvFbLMUuE)"
      ],
      "metadata": {
        "id": "4tIUvFbLMUuE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Let's teach AI writing like a Shakespeare ðŸŽ“"
      ],
      "metadata": {
        "id": "MofLJqBHAWXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Installing the model"
      ],
      "metadata": {
        "id": "W7wiPFGQQn9o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQACJ8lyUIR0",
        "outputId": "3e68ba16-9347-49df-f34c-74da9c7339fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gpt-2-simple in /usr/local/lib/python3.10/dist-packages (0.8.1)\n",
            "Requirement already satisfied: tensorflow>=2.5.1 in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (2.12.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (1.22.4)\n",
            "Requirement already satisfied: toposort in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (1.10)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.4.8)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.32.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (3.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.5.1->gpt-2-simple) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow>=2.5.1->gpt-2-simple) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow>=2.5.1->gpt-2-simple) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "#install the library we'll use today\n",
        "!pip install gpt-2-simple"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating text with basic model"
      ],
      "metadata": {
        "id": "ADzeFwzaQ8cT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing and loading necessary components"
      ],
      "metadata": {
        "id": "d6Ah3D1CRK6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import what we need\n",
        "import gpt_2_simple as gpt2 #for gpt-2 (our AI model)\n",
        "import os #lets us doing things with files and folders\n",
        "import requests #this one helps to dowload from the internet"
      ],
      "metadata": {
        "id": "mLg4pTPDaJJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#and let's download our AI model\n",
        "gpt2.download_gpt2()   # model is saved into current directory under /models/124M/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIXHjaxvaWsV",
        "outputId": "0b092916-c801-4327-d83f-323b26cad22a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 683Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 5.11Mit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 436Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:10, 45.7Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 788Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 6.39Mit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 6.85Mit/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#strating the session so we can play with the gpt-2 model\n",
        "sess = gpt2.start_tf_sess()"
      ],
      "metadata": {
        "id": "6CCkn75KbBpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we load the model from file to use it\n",
        "gpt2.load_gpt2(sess, run_name='124M', checkpoint_dir='models')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsBvHQsxZsyP",
        "outputId": "1a2873c9-2e06-484b-c7c0-1f4f6f809749"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text generation"
      ],
      "metadata": {
        "id": "mDSFDj78RQJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this is how we would start model statement\n",
        "prefix = \"Is there a second Earth?\""
      ],
      "metadata": {
        "id": "-P5_fxZOgGlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the model is generating text\n",
        "gpt2.generate(sess, run_name='124M', checkpoint_dir='models', prefix=prefix, length=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSYqTat0gNDo",
        "outputId": "41978740-7547-4658-c0df-6a03468eea0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is there a second Earth?\n",
            "\n",
            "I don't know. I don't think I can understand that. I mean, I'm not saying it's a planet, but it's a planet with a planet. At the end of the day, we don't know what happened\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating text with improved (finetuned) model"
      ],
      "metadata": {
        "id": "ML5helfmRjT0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT**\n",
        "</br>Restart the runtime (Runtime -> Restart runtime)"
      ],
      "metadata": {
        "id": "8cEaZKtRPx0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing and loading necessary components"
      ],
      "metadata": {
        "id": "NIPDKskeR7i3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import what we need\n",
        "import gpt_2_simple as gpt2 #for gpt-2 (our AI model)\n",
        "import os #lets us doing things with files and folders\n",
        "import requests #this one helps to dowload from the internet"
      ],
      "metadata": {
        "id": "eHys5-bWPnhJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get nietzsche texts\n",
        "!wget \"https://s3.amazonaws.com/text-datasets/nietzsche.txt\""
      ],
      "metadata": {
        "id": "dRTQyR7IqaOl",
        "outputId": "58a481cf-7c0b-452d-81ff-7a360491bd07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-24 11:37:34--  https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.90.22, 52.217.232.248, 54.231.228.176, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.90.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 600901 (587K) [text/plain]\n",
            "Saving to: â€˜nietzsche.txtâ€™\n",
            "\n",
            "nietzsche.txt       100%[===================>] 586.82K  1.34MB/s    in 0.4s    \n",
            "\n",
            "2023-05-24 11:37:35 (1.34 MB/s) - â€˜nietzsche.txtâ€™ saved [600901/600901]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#game of thrones from https://www.kaggle.com/datasets/khulasasndh/game-of-thrones-books?select=001ssb.txt\n",
        "!gdown \"1CrL1wde_NGO68i5Prd_UNA_oW0cGQsxg&confirm=t\"\n",
        "!mv /content/001ssb.txt /content/got1.txt"
      ],
      "metadata": {
        "id": "pzDNTjJzuKDW",
        "outputId": "b4b7a525-ff65-4e14-c6ea-39af72dcf0d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CrL1wde_NGO68i5Prd_UNA_oW0cGQsxg&confirm=t\n",
            "To: /content/001ssb.txt\n",
            "\r  0% 0.00/1.63M [00:00<?, ?B/s]\r100% 1.63M/1.63M [00:00<00:00, 78.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#let's dowload a file with all Shakespeare plays\n",
        "!wget \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "!mv /content/input.txt /content/shakespeare.txt"
      ],
      "metadata": {
        "id": "9pwWGn5eqBJn",
        "outputId": "4256c9ff-7a7a-4cea-ed06-4f6956bf9ce7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-24 11:37:43--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: â€˜input.txtâ€™\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-05-24 11:37:43 (50.4 MB/s) - â€˜input.txtâ€™ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#strating the session so we can play with the gpt-2 model\n",
        "sess = gpt2.start_tf_sess()"
      ],
      "metadata": {
        "id": "A0T2s8RxPnVr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Teaching our model"
      ],
      "metadata": {
        "id": "bvllQvFxR9z6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#finetuning with shakespeare.txt (which, to be honest, means that we are teaching the model how to write like a shakespeare)\n",
        "#it takes a lot of time (~15min)...\n",
        "gpt2.finetune(sess, 'got1.txt', steps=500)   # steps is max number of training steps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RJetxF6UOfY",
        "outputId": "d51988e5-8bcd-4012-bc73-89c8de1f2779"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n",
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.15s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset has 433157 tokens\n",
            "Training...\n",
            "[1 | 7.22] loss=3.51 avg=3.51\n",
            "[2 | 9.29] loss=3.37 avg=3.44\n",
            "[3 | 11.35] loss=3.41 avg=3.43\n",
            "[4 | 13.43] loss=3.29 avg=3.39\n",
            "[5 | 15.52] loss=3.26 avg=3.37\n",
            "[6 | 17.60] loss=3.38 avg=3.37\n",
            "[7 | 19.68] loss=3.34 avg=3.36\n",
            "[8 | 21.76] loss=3.35 avg=3.36\n",
            "[9 | 23.85] loss=3.26 avg=3.35\n",
            "[10 | 25.94] loss=3.11 avg=3.33\n",
            "[11 | 28.04] loss=3.25 avg=3.32\n",
            "[12 | 30.15] loss=3.08 avg=3.30\n",
            "[13 | 32.27] loss=3.24 avg=3.29\n",
            "[14 | 34.37] loss=3.22 avg=3.29\n",
            "[15 | 36.48] loss=3.14 avg=3.28\n",
            "[16 | 38.59] loss=3.19 avg=3.27\n",
            "[17 | 40.71] loss=3.16 avg=3.26\n",
            "[18 | 42.83] loss=3.33 avg=3.27\n",
            "[19 | 44.95] loss=3.09 avg=3.26\n",
            "[20 | 47.07] loss=3.20 avg=3.26\n",
            "[21 | 49.20] loss=3.12 avg=3.25\n",
            "[22 | 51.33] loss=3.17 avg=3.24\n",
            "[23 | 53.47] loss=3.17 avg=3.24\n",
            "[24 | 55.61] loss=3.04 avg=3.23\n",
            "[25 | 57.75] loss=3.24 avg=3.23\n",
            "[26 | 59.89] loss=2.98 avg=3.22\n",
            "[27 | 62.04] loss=3.08 avg=3.21\n",
            "[28 | 64.20] loss=2.99 avg=3.21\n",
            "[29 | 66.36] loss=2.89 avg=3.19\n",
            "[30 | 68.52] loss=2.98 avg=3.18\n",
            "[31 | 70.69] loss=2.96 avg=3.18\n",
            "[32 | 72.85] loss=3.11 avg=3.17\n",
            "[33 | 75.01] loss=3.03 avg=3.17\n",
            "[34 | 77.18] loss=3.02 avg=3.16\n",
            "[35 | 79.35] loss=3.05 avg=3.16\n",
            "[36 | 81.53] loss=3.13 avg=3.16\n",
            "[37 | 83.70] loss=2.86 avg=3.15\n",
            "[38 | 85.88] loss=3.12 avg=3.15\n",
            "[39 | 88.07] loss=3.00 avg=3.14\n",
            "[40 | 90.26] loss=2.95 avg=3.14\n",
            "[41 | 92.45] loss=3.00 avg=3.13\n",
            "[42 | 94.64] loss=3.00 avg=3.13\n",
            "[43 | 96.83] loss=3.05 avg=3.13\n",
            "[44 | 99.03] loss=3.12 avg=3.13\n",
            "[45 | 101.24] loss=3.06 avg=3.13\n",
            "[46 | 103.45] loss=3.07 avg=3.12\n",
            "[47 | 105.66] loss=2.95 avg=3.12\n",
            "[48 | 107.87] loss=3.09 avg=3.12\n",
            "[49 | 110.08] loss=3.03 avg=3.12\n",
            "[50 | 112.30] loss=2.95 avg=3.11\n",
            "[51 | 114.53] loss=3.02 avg=3.11\n",
            "[52 | 116.76] loss=2.97 avg=3.11\n",
            "[53 | 118.98] loss=3.06 avg=3.11\n",
            "[54 | 121.21] loss=2.97 avg=3.10\n",
            "[55 | 123.44] loss=3.07 avg=3.10\n",
            "[56 | 125.67] loss=3.09 avg=3.10\n",
            "[57 | 127.90] loss=2.92 avg=3.10\n",
            "[58 | 130.13] loss=2.93 avg=3.09\n",
            "[59 | 132.36] loss=2.98 avg=3.09\n",
            "[60 | 134.59] loss=2.81 avg=3.08\n",
            "[61 | 136.82] loss=2.92 avg=3.08\n",
            "[62 | 139.06] loss=3.01 avg=3.08\n",
            "[63 | 141.27] loss=3.02 avg=3.08\n",
            "[64 | 143.50] loss=2.91 avg=3.07\n",
            "[65 | 145.72] loss=3.03 avg=3.07\n",
            "[66 | 147.93] loss=2.94 avg=3.07\n",
            "[67 | 150.16] loss=2.82 avg=3.07\n",
            "[68 | 152.37] loss=2.99 avg=3.06\n",
            "[69 | 154.59] loss=2.96 avg=3.06\n",
            "[70 | 156.80] loss=2.93 avg=3.06\n",
            "[71 | 159.01] loss=2.89 avg=3.06\n",
            "[72 | 161.23] loss=2.93 avg=3.05\n",
            "[73 | 163.45] loss=2.81 avg=3.05\n",
            "[74 | 165.65] loss=2.82 avg=3.04\n",
            "[75 | 167.86] loss=2.84 avg=3.04\n",
            "[76 | 170.08] loss=2.76 avg=3.04\n",
            "[77 | 172.28] loss=3.04 avg=3.04\n",
            "[78 | 174.49] loss=2.77 avg=3.03\n",
            "[79 | 176.71] loss=2.92 avg=3.03\n",
            "[80 | 178.91] loss=2.94 avg=3.03\n",
            "[81 | 181.12] loss=2.78 avg=3.02\n",
            "[82 | 183.33] loss=2.88 avg=3.02\n",
            "[83 | 185.54] loss=2.88 avg=3.02\n",
            "[84 | 187.77] loss=2.77 avg=3.01\n",
            "[85 | 189.98] loss=2.88 avg=3.01\n",
            "[86 | 192.20] loss=2.99 avg=3.01\n",
            "[87 | 194.42] loss=2.89 avg=3.01\n",
            "[88 | 196.64] loss=2.86 avg=3.01\n",
            "[89 | 198.86] loss=2.97 avg=3.01\n",
            "[90 | 201.09] loss=2.79 avg=3.00\n",
            "[91 | 203.32] loss=2.78 avg=3.00\n",
            "[92 | 205.54] loss=2.84 avg=3.00\n",
            "[93 | 207.75] loss=2.89 avg=2.99\n",
            "[94 | 209.98] loss=2.88 avg=2.99\n",
            "[95 | 212.21] loss=2.89 avg=2.99\n",
            "[96 | 214.44] loss=2.72 avg=2.99\n",
            "[97 | 216.66] loss=2.95 avg=2.99\n",
            "[98 | 218.88] loss=2.66 avg=2.98\n",
            "[99 | 221.10] loss=2.83 avg=2.98\n",
            "[100 | 223.33] loss=2.83 avg=2.98\n",
            "======== SAMPLE 1 ========\n",
            " time, \"There she is. \"\n",
            "The boy gave her a look that said, \"My son.\" He pushed himself to his feet, \"This is your wife, my son.\" He held out his hand while the girl in the blanket rubbed her hair. He pressed one hand around her throat. \"She likes these, so do I.\" \"Amber is a good girl.\" The girl had a pale blue eyes and a black \n",
            "whisker on her cheek. She looked at him with a pale face and said, \"You want to go?\" \n",
            "\"No,\" the boy said, angry. \n",
            "A pale black face looked at her. It had red eyes and a blackish-bronze mouth. The first thing she noticed was his angry \n",
            "black face. It would be fine if the gods wouldn't curse him. \"No,\" I repeated. \n",
            "\"They curse a lot. I thought that was a curse and it was not. The man in the blanket, he's no knight, he was not a Lord of \n",
            "the Seven. Gods, he's no Knight of the Lance. Why were the wolves attacking me? The gods, the gods. Gods did they \n",
            "know, they say. What was this? No one did it, I think.\" His voice was deep as rainbows. \"Amber, how are you?\" \n",
            "When she said, \"I'm fine, my son.\" He looked at her uncertainly. \"You're a fool, my \n",
            "son, I can't hold it in. It's been three days, I promised she wouldn't hurt me. I'm a good girl, no \n",
            "murderer, I'll bring the boy to her bed when she wakes and give me milk.\" A pale white light streamed \n",
            "through the \n",
            "eyes of his wife and he saw her eyes widen in shock. \"The gods are cruel, your wife!\" \n",
            "\"The gods do not harm men,\" she was saying. The words grew colder, more desperate. \"The gods \n",
            "are a curse on men, even here. They curse the Gods, I told you, on me. I told you they were \n",
            "the curse, and they curse them now, so tell me this.\" \n",
            "\"The man in the blanket, he's no knight, and his wife said I looked down on them.\" \n",
            "\"No, the one in the blanket,\" his wife said. \"What does what?\" \n",
            "A pale boy in the blanket looked up at her, and shook his head. \"I was wrong,\" he said again, \n",
            "and suddenly there was a noise like a hammer, and he heard a noise like a pounding on a drum, a beating, and that \n",
            "sound of the hammer hitting the ground. \n",
            "For him, she was screaming, for him, for all the world. He was screaming. \n",
            "It was sudden, almost sudden. \"Amber, get down to the ground,\" the boy said loudly. He pushed himself off the blanket, the \n",
            "crying children's voices whispering it to him. The blankets were a blanket of tears, but somehow it was not too \n",
            "much. \n",
            "She was crying, but they had never touched her like that in her whole life. \"No!\" she sobbed, trying to \n",
            "speak, but she found that silence only to speak. What was she to do? \"Come on,\" she cried again, but that had only lasted \n",
            "one second. \n",
            "\"I said, come down to the ground!\" The boy raised his voice. \"It went down to the ground.\" He \n",
            "crouched in his hands and walked backward. He stopped a foot or two from the blanket, his hands in his \n",
            "flesh. The tears started to run down his face and down his cheek and up his arms. That was when he heard his \n",
            "voice. \"Amber, come down. Make her come down.\" The scream was still. Her hand trembled, but he \n",
            "made no sound. \n",
            "He sat on the floor, and they touched her. His hand fell backward, and she was screaming, for \n",
            "her. What else could she do, but to hear that! Where should she go? \"Come down, go down,\" the boy whispered. \n",
            "Amber was crying again. She heard him. The tears ran down his face. Her hand trembled, but he \n",
            "made no sound. She heard him. \"Amber, come down. Make her come down.\" The scream was still. \n",
            "Finally he sat down on the floor and her hand trembled again. He sat on the floor, on top of the bed, his \n",
            "hands on his heads as the boy walked. He lay, on the floor, weeping. The tears stopped, and he \n",
            "made no sound. He stood up and began to cry. The boy came back to him, and\n",
            "\n",
            "[101 | 238.47] loss=2.78 avg=2.97\n",
            "[102 | 240.69] loss=2.92 avg=2.97\n",
            "[103 | 242.91] loss=2.73 avg=2.97\n",
            "[104 | 245.14] loss=2.73 avg=2.96\n",
            "[105 | 247.37] loss=2.87 avg=2.96\n",
            "[106 | 249.60] loss=2.86 avg=2.96\n",
            "[107 | 251.82] loss=2.69 avg=2.96\n",
            "[108 | 254.05] loss=2.95 avg=2.96\n",
            "[109 | 256.28] loss=2.72 avg=2.95\n",
            "[110 | 258.51] loss=2.80 avg=2.95\n",
            "[111 | 260.73] loss=2.82 avg=2.95\n",
            "[112 | 262.96] loss=2.89 avg=2.95\n",
            "[113 | 265.19] loss=2.83 avg=2.95\n",
            "[114 | 267.41] loss=2.70 avg=2.94\n",
            "[115 | 269.64] loss=2.77 avg=2.94\n",
            "[116 | 271.86] loss=2.73 avg=2.94\n",
            "[117 | 274.10] loss=2.81 avg=2.94\n",
            "[118 | 276.32] loss=2.62 avg=2.93\n",
            "[119 | 278.55] loss=2.81 avg=2.93\n",
            "[120 | 280.78] loss=2.77 avg=2.93\n",
            "[121 | 283.00] loss=2.61 avg=2.92\n",
            "[122 | 285.23] loss=2.77 avg=2.92\n",
            "[123 | 287.46] loss=2.79 avg=2.92\n",
            "[124 | 289.68] loss=2.73 avg=2.92\n",
            "[125 | 291.90] loss=2.71 avg=2.91\n",
            "[126 | 294.12] loss=2.83 avg=2.91\n",
            "[127 | 296.35] loss=2.71 avg=2.91\n",
            "[128 | 298.58] loss=2.61 avg=2.90\n",
            "[129 | 300.80] loss=2.73 avg=2.90\n",
            "[130 | 303.02] loss=2.64 avg=2.90\n",
            "[131 | 305.24] loss=2.86 avg=2.90\n",
            "[132 | 307.46] loss=2.65 avg=2.89\n",
            "[133 | 309.68] loss=2.83 avg=2.89\n",
            "[134 | 311.91] loss=2.70 avg=2.89\n",
            "[135 | 314.13] loss=2.92 avg=2.89\n",
            "[136 | 316.35] loss=2.65 avg=2.89\n",
            "[137 | 318.57] loss=2.69 avg=2.89\n",
            "[138 | 320.80] loss=2.82 avg=2.89\n",
            "[139 | 323.02] loss=2.61 avg=2.88\n",
            "[140 | 325.25] loss=2.74 avg=2.88\n",
            "[141 | 327.47] loss=2.52 avg=2.87\n",
            "[142 | 329.69] loss=2.68 avg=2.87\n",
            "[143 | 331.91] loss=2.75 avg=2.87\n",
            "[144 | 334.13] loss=2.67 avg=2.87\n",
            "[145 | 336.36] loss=2.58 avg=2.86\n",
            "[146 | 338.58] loss=2.78 avg=2.86\n",
            "[147 | 340.80] loss=2.65 avg=2.86\n",
            "[148 | 343.02] loss=2.56 avg=2.86\n",
            "[149 | 345.24] loss=2.81 avg=2.86\n",
            "[150 | 347.46] loss=2.86 avg=2.86\n",
            "[151 | 349.68] loss=2.78 avg=2.85\n",
            "[152 | 351.89] loss=2.72 avg=2.85\n",
            "[153 | 354.11] loss=2.66 avg=2.85\n",
            "[154 | 356.32] loss=2.63 avg=2.85\n",
            "[155 | 358.55] loss=2.78 avg=2.85\n",
            "[156 | 360.78] loss=2.70 avg=2.85\n",
            "[157 | 363.01] loss=2.64 avg=2.84\n",
            "[158 | 365.23] loss=2.48 avg=2.84\n",
            "[159 | 367.44] loss=2.74 avg=2.84\n",
            "[160 | 369.67] loss=2.55 avg=2.83\n",
            "[161 | 371.89] loss=2.63 avg=2.83\n",
            "[162 | 374.11] loss=2.67 avg=2.83\n",
            "[163 | 376.34] loss=2.59 avg=2.83\n",
            "[164 | 378.57] loss=2.68 avg=2.82\n",
            "[165 | 380.79] loss=2.87 avg=2.82\n",
            "[166 | 383.02] loss=2.87 avg=2.83\n",
            "[167 | 385.25] loss=2.53 avg=2.82\n",
            "[168 | 387.47] loss=2.83 avg=2.82\n",
            "[169 | 389.69] loss=2.67 avg=2.82\n",
            "[170 | 391.91] loss=2.64 avg=2.82\n",
            "[171 | 394.13] loss=2.61 avg=2.82\n",
            "[172 | 396.36] loss=2.82 avg=2.82\n",
            "[173 | 398.58] loss=2.68 avg=2.81\n",
            "[174 | 400.81] loss=2.56 avg=2.81\n",
            "[175 | 403.03] loss=2.53 avg=2.81\n",
            "[176 | 405.25] loss=2.71 avg=2.81\n",
            "[177 | 407.47] loss=2.70 avg=2.80\n",
            "[178 | 409.70] loss=2.62 avg=2.80\n",
            "[179 | 411.92] loss=2.92 avg=2.80\n",
            "[180 | 414.14] loss=2.62 avg=2.80\n",
            "[181 | 416.37] loss=2.63 avg=2.80\n",
            "[182 | 418.58] loss=2.60 avg=2.80\n",
            "[183 | 420.80] loss=2.42 avg=2.79\n",
            "[184 | 423.03] loss=2.63 avg=2.79\n",
            "[185 | 425.25] loss=2.58 avg=2.79\n",
            "[186 | 427.47] loss=2.64 avg=2.79\n",
            "[187 | 429.69] loss=2.73 avg=2.79\n",
            "[188 | 431.92] loss=2.51 avg=2.78\n",
            "[189 | 434.14] loss=2.51 avg=2.78\n",
            "[190 | 436.36] loss=2.65 avg=2.78\n",
            "[191 | 438.58] loss=2.76 avg=2.78\n",
            "[192 | 440.81] loss=2.59 avg=2.78\n",
            "[193 | 443.03] loss=2.48 avg=2.77\n",
            "[194 | 445.25] loss=2.47 avg=2.77\n",
            "[195 | 447.49] loss=2.61 avg=2.77\n",
            "[196 | 449.71] loss=2.63 avg=2.77\n",
            "[197 | 451.93] loss=2.56 avg=2.76\n",
            "[198 | 454.15] loss=2.46 avg=2.76\n",
            "[199 | 456.37] loss=2.67 avg=2.76\n",
            "[200 | 458.60] loss=2.73 avg=2.76\n",
            "======== SAMPLE 1 ========\n",
            " to be . But she had to. It did not seem like a choice. You were not to be harmed, it seems to me.\" She closed her eyes, breathing hard. \"The moment I saw that baby, . . . it broke in pieces. My fingers were numb, my fingers were numb, and a deep orange was crawling into my eyes, and . . . it was going to hurt. You could not move \n",
            "your hand . . . a finger cannot do anything. My arm was dead, \n",
            "so I lay there on my bed, trembling. . . but I could feel blood on my arm. I could feel the blade of water \n",
            "on it's side, filling my arm. It was \n",
            "there at last I remembered . . . and the moment I opened my eyes I saw the same blood, and how it was \n",
            "Page 39\n",
            "\n",
            ". . . it was running down my arm.\" She opened her eyes again. \n",
            "V \n",
            "\"You're not hurting her, aren't you?\" The baby's voice came from behind her, and when she looked back, the \n",
            "lover's eyes were on her. \"No. You're hurting her.\" It had gotten to him. It seemed as if she \n",
            "was falling off of a mountain of pain and rage. He would have died if this wasn't the end. He took his hold of her \n",
            "and pulled her down, and then he said, \"V' \n",
            "\"Take her.\" \n",
            "He put his hand on the bed and gently twisted it, lifting it to try and \n",
            "prove it. \n",
            "So he did, so that she went over to the bedchamber and pulled out the dagger . . . \n",
            "she had fallen from the mountain of pain and rage and all the grief all around her. \n",
            "She could not keep her eye on him, she knew, but if you're a girl, he could see that you're dying. There \n",
            "were two dark \n",
            "eyes on his dead hand, with black beards and silver-gold earls. \"I would . . . I . . . \" she was \n",
            "utterly terrified, but the thought seemed to bring out the fear within her. \n",
            "\"V' \n",
            "It was the last last word he said and said so, so fast his fingers dug into her flesh like a shaft \n",
            "of water in the bath. There was blood all around her, and the pain was rawer than any poison he \n",
            "had ever felt. There was blood on her cheeks, where the little finger had been, and on her arm, and around her \n",
            "heart. So quick that he could not hope to take her back to him, he whispered something to her in \n",
            "audible whispers, as the pain rose and went, then went down as well, with the blade \n",
            "still there in her hand. The pain was so real her skin grew red. When it was done, he moved to the \n",
            "medicine table and made a small talk with her, saying, \"I want to feel your pain.\" The pain was so real \n",
            "in her that she thought he was telling her a lie, and so it hurt to lie to a stranger. All that did was make him \n",
            "believe whatever she told him. \n",
            "It took Varys until last week to find the child. He came by her as soon as they arrived \n",
            "and took her from the \n",
            "bed, pulling her to the bedding. He'd used a knife and broken the boy's fingers on both \n",
            "hands. \n",
            "When she woke, he was lying on his stomach, bloody and swollen, his legs hurting so badly she had \n",
            "to get him up and climb his back with his legs so wide, so tight. The nurse would not tell him what \n",
            "he was \n",
            "complaining about, but when Varys pulled him back into the bed and helped his leg to the \n",
            "bedspread and moved her face closer, she realized the pain had been so deep and \n",
            "horrible that he now called her by her last name, and she cursed in her heart. Varys \n",
            "showed the child how to put a cloth over her elbow and tell her when \n",
            "to put back her gloves. \"If this is what you were saying, sweet child, it won't do to want another time. Please \n",
            "take this one time. We ought to have brought a knife. You might have found me a more soothing \n",
            "woman, but you'd want to learn how to put this one time.\" \n",
            "She had no choice, she said, but the child needed to go. She was a small girl, yet she needed her pain to \n",
            "come as a relief. It was not something she was certain she could do. \"I'm sorry,\" he said, \"but I \n",
            "wondered if this was my last time seeing you\n",
            "\n",
            "[201 | 471.78] loss=2.36 avg=2.75\n",
            "[202 | 474.01] loss=2.60 avg=2.75\n",
            "[203 | 476.23] loss=2.38 avg=2.75\n",
            "[204 | 478.45] loss=2.47 avg=2.74\n",
            "[205 | 480.66] loss=2.32 avg=2.74\n",
            "[206 | 482.89] loss=2.71 avg=2.74\n",
            "[207 | 485.11] loss=2.62 avg=2.74\n",
            "[208 | 487.33] loss=2.61 avg=2.74\n",
            "[209 | 489.56] loss=2.35 avg=2.73\n",
            "[210 | 491.77] loss=2.64 avg=2.73\n",
            "[211 | 493.99] loss=2.53 avg=2.73\n",
            "[212 | 496.22] loss=2.69 avg=2.73\n",
            "[213 | 498.45] loss=2.74 avg=2.73\n",
            "[214 | 500.67] loss=2.51 avg=2.73\n",
            "[215 | 502.89] loss=2.63 avg=2.72\n",
            "[216 | 505.11] loss=2.67 avg=2.72\n",
            "[217 | 507.34] loss=2.39 avg=2.72\n",
            "[218 | 509.58] loss=2.45 avg=2.72\n",
            "[219 | 511.80] loss=2.67 avg=2.72\n",
            "[220 | 514.03] loss=2.61 avg=2.72\n",
            "[221 | 516.25] loss=2.53 avg=2.71\n",
            "[222 | 518.48] loss=2.36 avg=2.71\n",
            "[223 | 520.71] loss=2.70 avg=2.71\n",
            "[224 | 522.93] loss=2.80 avg=2.71\n",
            "[225 | 525.15] loss=2.24 avg=2.71\n",
            "[226 | 527.38] loss=2.55 avg=2.70\n",
            "[227 | 529.60] loss=2.66 avg=2.70\n",
            "[228 | 531.83] loss=2.56 avg=2.70\n",
            "[229 | 534.06] loss=2.38 avg=2.70\n",
            "[230 | 536.29] loss=2.47 avg=2.70\n",
            "[231 | 538.51] loss=2.55 avg=2.69\n",
            "[232 | 540.73] loss=2.55 avg=2.69\n",
            "[233 | 542.96] loss=2.47 avg=2.69\n",
            "[234 | 545.18] loss=2.45 avg=2.69\n",
            "[235 | 547.41] loss=2.43 avg=2.68\n",
            "[236 | 549.63] loss=2.57 avg=2.68\n",
            "[237 | 551.86] loss=2.60 avg=2.68\n",
            "[238 | 554.08] loss=2.46 avg=2.68\n",
            "[239 | 556.31] loss=2.64 avg=2.68\n",
            "[240 | 558.54] loss=2.44 avg=2.68\n",
            "[241 | 560.76] loss=2.50 avg=2.67\n",
            "[242 | 562.99] loss=2.63 avg=2.67\n",
            "[243 | 565.21] loss=2.40 avg=2.67\n",
            "[244 | 567.43] loss=2.73 avg=2.67\n",
            "[245 | 569.66] loss=2.64 avg=2.67\n",
            "[246 | 571.88] loss=2.37 avg=2.67\n",
            "[247 | 574.10] loss=2.57 avg=2.67\n",
            "[248 | 576.32] loss=2.59 avg=2.67\n",
            "[249 | 578.55] loss=2.32 avg=2.66\n",
            "[250 | 580.77] loss=2.51 avg=2.66\n",
            "[251 | 583.00] loss=2.24 avg=2.66\n",
            "[252 | 585.22] loss=2.66 avg=2.66\n",
            "[253 | 587.44] loss=2.57 avg=2.66\n",
            "[254 | 589.66] loss=2.55 avg=2.65\n",
            "[255 | 591.89] loss=2.28 avg=2.65\n",
            "[256 | 594.12] loss=2.41 avg=2.65\n",
            "[257 | 596.34] loss=2.35 avg=2.64\n",
            "[258 | 598.56] loss=2.49 avg=2.64\n",
            "[259 | 600.79] loss=2.52 avg=2.64\n",
            "[260 | 603.01] loss=2.38 avg=2.64\n",
            "[261 | 605.23] loss=2.39 avg=2.64\n",
            "[262 | 607.46] loss=2.39 avg=2.63\n",
            "[263 | 609.68] loss=2.60 avg=2.63\n",
            "[264 | 611.91] loss=2.58 avg=2.63\n",
            "[265 | 614.14] loss=2.34 avg=2.63\n",
            "[266 | 616.36] loss=2.62 avg=2.63\n",
            "[267 | 618.60] loss=2.46 avg=2.63\n",
            "[268 | 620.82] loss=2.51 avg=2.63\n",
            "[269 | 623.04] loss=2.24 avg=2.62\n",
            "[270 | 625.26] loss=2.38 avg=2.62\n",
            "[271 | 627.48] loss=2.15 avg=2.61\n",
            "[272 | 629.71] loss=2.17 avg=2.61\n",
            "[273 | 631.94] loss=2.30 avg=2.61\n",
            "[274 | 634.17] loss=2.42 avg=2.60\n",
            "[275 | 636.39] loss=2.34 avg=2.60\n",
            "[276 | 638.61] loss=2.29 avg=2.60\n",
            "[277 | 640.83] loss=2.57 avg=2.60\n",
            "[278 | 643.06] loss=2.41 avg=2.60\n",
            "[279 | 645.29] loss=2.38 avg=2.59\n",
            "[280 | 647.51] loss=2.33 avg=2.59\n",
            "[281 | 649.74] loss=2.41 avg=2.59\n",
            "[282 | 651.96] loss=2.26 avg=2.58\n",
            "[283 | 654.18] loss=2.13 avg=2.58\n",
            "[284 | 656.41] loss=2.17 avg=2.58\n",
            "[285 | 658.64] loss=2.25 avg=2.57\n",
            "[286 | 660.86] loss=2.38 avg=2.57\n",
            "[287 | 663.08] loss=2.52 avg=2.57\n",
            "[288 | 665.31] loss=2.49 avg=2.57\n",
            "[289 | 667.53] loss=2.27 avg=2.57\n",
            "[290 | 669.76] loss=2.19 avg=2.56\n",
            "[291 | 671.98] loss=2.34 avg=2.56\n",
            "[292 | 674.20] loss=2.25 avg=2.56\n",
            "[293 | 676.43] loss=2.33 avg=2.55\n",
            "[294 | 678.65] loss=2.55 avg=2.55\n",
            "[295 | 680.89] loss=2.30 avg=2.55\n",
            "[296 | 683.11] loss=2.64 avg=2.55\n",
            "[297 | 685.34] loss=2.30 avg=2.55\n",
            "[298 | 687.57] loss=2.43 avg=2.55\n",
            "[299 | 689.78] loss=2.68 avg=2.55\n",
            "[300 | 692.01] loss=2.38 avg=2.55\n",
            "======== SAMPLE 1 ========\n",
            " man and all his household shit-kickers. \n",
            "Robert's horse was no more than a squat horse. Its back was twisted in \n",
            "surrounding bands that stretched from shoulder to wrist, its helm \n",
            "scattered everywhere, thick as stone. That was where he found Mormont, the \n",
            "man who rode with him. He was taller than Robert, perhaps even more frightening. He stood \n",
            "with a wiry man with a wiry frame, with the thick brown hair of the east-guard, his face a deep rue \n",
            "of years of courtly dishonor, and his face, if the boy thought he was a fool, was almost of a \n",
            "man's, full of mischief. His coat was gold, golden with gold, and draped with a crowned eagle. \n",
            "\"Lord Tywin would be a lord no man of Lannister would dare meet,\" he proclaimed to Varys. \n",
            "\"His brother Robert would scarcely give him a word.\" \n",
            "\"Robert is the Lord Steward. His father was a brother of Lord Stannis Baratheon, but I see no \n",
            "betrothing that here. I would doubtmuch that his brother Stannis was the last man to ride into battle with a \n",
            "tent. I saw Stannis again . . . and by a man who has lived to be three hundred of his years. Robert \n",
            "has served his realm in two hundred years. Stannis is another matter altogether. His brother Lord \n",
            "Stane was never more than an adult at best, and as such was little more than a child on \n",
            "the wrong side of eighteen. I doubt that Stannis will rule for many more years. He will surely play that \n",
            "ball himself and be as dead as Eddard Stark.\" \n",
            "Page 241\n",
            "\n",
            "Varys frowned. \"As I said, Lord Stannis is no friend of mine. I would welcome his welcome, in \n",
            "principle or otherwise, but as all men do in wars.\" \n",
            "\"Well, I fear Lord Stannis would be an unsuitable man for command. Perhaps he will have little interest in \n",
            "combat,\" Varys said. \"And as men do in the service of their lord, we ought to welcome him with open arms, \n",
            "not fret over his devotion to us in the name of a well-being.\" \n",
            "Varys said, firmly, \"I shall not fret over devotion, that is known as the man's nature. I \n",
            "will gladly give him the green light to join the Night's Watch.\" \n",
            "Robert looked at his brother in surprise. \"That was just my brother.\" Robert had never been a Stark. He had \n",
            "never been a lord. He had never joined the Starks. \"Lord Stannis has said he wishes to go to Stannis, \n",
            "but, Lord Stark, I will find a place for you in the council.\" \n",
            "\"No council,\" Robert protested. \"Lord Stannis is not my brother. Nor the man. And when he does come here, I \n",
            "will ask you and your sons to keep him away. I need no councillors, my lords.\" His voice was stiff and \n",
            "mockery. \"I had the right to speak at council. I am a man of the realm and a councillor has no business \n",
            "discarding the decisions of a council.\" \n",
            "The Lord of the Eyrie's counsel was the same. \"You and your councillors, in truth, are the ones I must \n",
            "promise.\" \n",
            "And Robert did want to go. He told Varys he was the better man for it. \"The honor pays for such \n",
            "promises,\" he added; \"but that is only what a councillor can do. A seat does not require a seat. \n",
            "Lord Hoster, you command the council, I hope . . . not you, but you, and your brother Stannis, what honor can \n",
            "you give a council? You command a council? What are you waiting for, Lord Stannis? To be your lord, \n",
            "no lord is equal to justice. We do not need another Eddard Stark. This time he will have a look like \n",
            "him, I promise you.\" \n",
            "\"And who do you promise us?\" Robert asked with a bitter look on his face. His black eyes were \n",
            "obsessed with rage now. \"The honor. I command you. The honor of the Night's Watch. Honor . . . but you, \n",
            "you do not believe me?\" \n",
            "\"I will not believe you, Robert,\" Varys said. \"Even if you do. I know the truth of this. My brother Stannis is not here \n",
            "to help us win a war. He is in the service of Lannister kings, and I know he might not understand \n",
            "me if he saw something I did. If I cannot\n",
            "\n",
            "[301 | 705.30] loss=2.24 avg=2.54\n",
            "[302 | 707.52] loss=2.10 avg=2.54\n",
            "[303 | 709.74] loss=2.23 avg=2.54\n",
            "[304 | 711.96] loss=2.17 avg=2.53\n",
            "[305 | 714.18] loss=2.30 avg=2.53\n",
            "[306 | 716.40] loss=2.41 avg=2.53\n",
            "[307 | 718.63] loss=2.25 avg=2.53\n",
            "[308 | 720.85] loss=2.26 avg=2.52\n",
            "[309 | 723.08] loss=2.33 avg=2.52\n",
            "[310 | 725.29] loss=2.11 avg=2.52\n",
            "[311 | 727.51] loss=2.14 avg=2.51\n",
            "[312 | 729.74] loss=2.18 avg=2.51\n",
            "[313 | 731.96] loss=2.34 avg=2.51\n",
            "[314 | 734.18] loss=2.19 avg=2.50\n",
            "[315 | 736.41] loss=2.38 avg=2.50\n",
            "[316 | 738.63] loss=2.35 avg=2.50\n",
            "[317 | 740.86] loss=2.03 avg=2.50\n",
            "[318 | 743.08] loss=2.40 avg=2.50\n",
            "[319 | 745.31] loss=2.21 avg=2.49\n",
            "[320 | 747.53] loss=2.21 avg=2.49\n",
            "[321 | 749.74] loss=2.17 avg=2.49\n",
            "[322 | 751.97] loss=2.09 avg=2.48\n",
            "[323 | 754.19] loss=2.20 avg=2.48\n",
            "[324 | 756.42] loss=2.22 avg=2.48\n",
            "[325 | 758.64] loss=2.05 avg=2.47\n",
            "[326 | 760.86] loss=2.14 avg=2.47\n",
            "[327 | 763.08] loss=2.17 avg=2.47\n",
            "[328 | 765.31] loss=2.23 avg=2.46\n",
            "[329 | 767.54] loss=2.41 avg=2.46\n",
            "[330 | 769.76] loss=2.32 avg=2.46\n",
            "[331 | 771.98] loss=2.11 avg=2.46\n",
            "[332 | 774.20] loss=2.47 avg=2.46\n",
            "[333 | 776.42] loss=2.24 avg=2.46\n",
            "[334 | 778.65] loss=2.55 avg=2.46\n",
            "[335 | 780.86] loss=2.04 avg=2.45\n",
            "[336 | 783.08] loss=2.35 avg=2.45\n",
            "[337 | 785.30] loss=2.24 avg=2.45\n",
            "[338 | 787.52] loss=2.12 avg=2.45\n",
            "[339 | 789.74] loss=2.04 avg=2.44\n",
            "[340 | 791.96] loss=2.38 avg=2.44\n",
            "[341 | 794.18] loss=2.36 avg=2.44\n",
            "[342 | 796.41] loss=2.40 avg=2.44\n",
            "[343 | 798.63] loss=2.25 avg=2.44\n",
            "[344 | 800.85] loss=2.38 avg=2.44\n",
            "[345 | 803.08] loss=2.25 avg=2.44\n",
            "[346 | 805.30] loss=2.19 avg=2.43\n",
            "[347 | 807.52] loss=2.23 avg=2.43\n",
            "[348 | 809.74] loss=2.31 avg=2.43\n",
            "[349 | 811.97] loss=2.22 avg=2.43\n",
            "[350 | 814.19] loss=2.22 avg=2.42\n",
            "[351 | 816.42] loss=2.30 avg=2.42\n",
            "[352 | 818.64] loss=2.10 avg=2.42\n",
            "[353 | 820.86] loss=2.23 avg=2.42\n",
            "[354 | 823.08] loss=2.29 avg=2.42\n",
            "[355 | 825.31] loss=2.06 avg=2.41\n",
            "[356 | 827.53] loss=2.18 avg=2.41\n",
            "[357 | 829.75] loss=2.20 avg=2.41\n",
            "[358 | 831.98] loss=2.18 avg=2.41\n",
            "[359 | 834.20] loss=1.99 avg=2.40\n",
            "[360 | 836.42] loss=1.99 avg=2.40\n",
            "[361 | 838.65] loss=2.02 avg=2.39\n",
            "[362 | 840.88] loss=2.05 avg=2.39\n",
            "[363 | 843.10] loss=1.91 avg=2.39\n",
            "[364 | 845.33] loss=2.18 avg=2.38\n",
            "[365 | 847.55] loss=2.06 avg=2.38\n",
            "[366 | 849.77] loss=2.09 avg=2.38\n",
            "[367 | 852.00] loss=2.05 avg=2.37\n",
            "[368 | 854.23] loss=2.09 avg=2.37\n",
            "[369 | 856.45] loss=2.10 avg=2.37\n",
            "[370 | 858.67] loss=2.04 avg=2.36\n",
            "[371 | 860.89] loss=1.99 avg=2.36\n",
            "[372 | 863.12] loss=2.35 avg=2.36\n",
            "[373 | 865.35] loss=1.89 avg=2.36\n",
            "[374 | 867.57] loss=2.09 avg=2.35\n",
            "[375 | 869.79] loss=1.88 avg=2.35\n",
            "[376 | 872.01] loss=2.16 avg=2.35\n",
            "[377 | 874.24] loss=2.24 avg=2.35\n",
            "[378 | 876.46] loss=2.23 avg=2.34\n",
            "[379 | 878.69] loss=1.98 avg=2.34\n",
            "[380 | 880.91] loss=1.90 avg=2.34\n",
            "[381 | 883.13] loss=2.19 avg=2.33\n",
            "[382 | 885.36] loss=2.13 avg=2.33\n",
            "[383 | 887.58] loss=2.11 avg=2.33\n",
            "[384 | 889.81] loss=2.41 avg=2.33\n",
            "[385 | 892.04] loss=2.00 avg=2.33\n",
            "[386 | 894.26] loss=2.14 avg=2.33\n",
            "[387 | 896.49] loss=1.98 avg=2.32\n",
            "[388 | 898.72] loss=1.92 avg=2.32\n",
            "[389 | 900.94] loss=2.00 avg=2.31\n",
            "[390 | 903.17] loss=1.94 avg=2.31\n",
            "[391 | 905.39] loss=2.10 avg=2.31\n",
            "[392 | 907.61] loss=2.09 avg=2.31\n",
            "[393 | 909.83] loss=1.92 avg=2.30\n",
            "[394 | 912.05] loss=2.11 avg=2.30\n",
            "[395 | 914.28] loss=2.13 avg=2.30\n",
            "[396 | 916.51] loss=1.94 avg=2.30\n",
            "[397 | 918.73] loss=2.13 avg=2.29\n",
            "[398 | 920.96] loss=2.30 avg=2.29\n",
            "[399 | 923.18] loss=1.83 avg=2.29\n",
            "[400 | 925.40] loss=2.01 avg=2.29\n",
            "======== SAMPLE 1 ========\n",
            " a dragon. \n",
            "She had seen the faces of those around him in the hall. Ser Kevan, Daryn, Ser Meryn, Ser Loras. All of them . . . but not even \n",
            "Ser Loras was a man like this when the Greatjon Tully had him slain, or even a man like that . . . no doubt \n",
            "the Hound had seen fit to slay him himself. Yet if there was some \n",
            "hollow being watching over her, it was that one who had dared stand before her with those eyes. \n",
            "\"I cannot believe it,\" she muttered. \"You must be the one who said it . . .\" \n",
            "\"Is she?\" Ser Boros asked the maester. \n",
            "Page 463\n",
            "\n",
            "CATELYN \n",
            "It seemed to Catelyn that there was nothing like the sound of the queen's voice to stir her heart. And the \n",
            "seagulls, she whispered. The green eyes watched her sleep. \n",
            "Catelyn woke, startled. The sound of their voices was so strong she treed her head back \n",
            "and saw the sunburst tree branch crack underfoot. It was the northern stag. When she glanced up, it was \n",
            "Robb, taller than Catelyn, with an ugly scar above his head. He was the first man she had seen in years, \n",
            "almost twenty years. Her mother thought that might have been him looking so bad. \n",
            "The sunburst tree branched from its trunk and wove a long web around the king. \"Come here, my lady,\" the \n",
            "lord said. He was a fat man with a heavy beard, with flecked hair and a black eye. His cape was black \n",
            "and the cloak white. The queen had taught him to dress red. \"He's not even a stag,\" she told him as \n",
            "she pulled her silver cape back on her back to keep him from falling off his horse. \"He's a real man. The \n",
            "stag in this castle is so ugly I'd give him a Lannister look.\" She was wrong. \n",
            "Robb's sword was black and his cape white, and the sigil was a direwolf direwolf. His cape was \n",
            "black and the cloak white. He was dressed in a red cloak and doublet-leather armor, with a black wolf \n",
            "crest around his neck and a heavy golden sword in his hand. \"Is this some kind of trick, my lady? Is \n",
            "I supposed to go hunting now?\" The queen had been all grown up before her time. And while she \n",
            "kept silent, Tywin Lannister was a hundred times more cunning than Jaime Lannister. \n",
            "So Catelyn wondered what had happened to Robb. \"Is it that terrible of a time, my lord?\" she asked. \n",
            "\"It's not as bad as it sounds,\" Tywin replied. \"Lord Eddard is almost a man grown for his age and \n",
            "whole body. The rest of you are grown-up. Let me tell you how I grew my legs when I was a boy, and \n",
            "I still grow my leg now.\" \n",
            "\"I've grown my legs,\" Catelyn told him. \n",
            "\"Yes, of course . . . the ones at this moment. It is very dark, do you think? There is no sunlight. We \n",
            "can see no stars, only the moon and the winds. You are growing a thousand hairs a minute on your hind legs . . .\" \n",
            "\"In my dream, you will always be a man,\" Catelyn said uncertainly. \"The thing is, we will never know the \n",
            "s.o. that grows.\" \n",
            "\"In your dream we will have men like you. What good is that, Lordling? You will be a commander in \n",
            "your lords' banner and wear the lion's share of honor. You will be as hard as any bastard I have ever \n",
            "seen . . . but then-\" \n",
            "Catelyn stopped. Tears streamed down her face. \n",
            "\"You hate it,\" Tywin Lannister said. His voice was full of sadness. \"Did Jon Snow help that one?\" \n",
            "\"Not Jon. He's still my brother. He was my own lord father, and heir to the Trident. You know that, \n",
            "sweet sister.\" \n",
            "It was so true. \"So it is, in my dream. I mean, it is true, yes, yes. I hate losing him. He's \n",
            "the Targaryen we know now. We'll see where He ends and Targaryen begins, for once. He \n",
            "was my brother's bannerman when he took the Black Goat.\" \n",
            "The queen had known that for years. Yet now it was more than she'd ever known. \n",
            "Catelyn was a little startled. \"And\n",
            "\n",
            "[401 | 938.97] loss=1.96 avg=2.28\n",
            "[402 | 941.19] loss=2.02 avg=2.28\n",
            "[403 | 943.41] loss=2.16 avg=2.28\n",
            "[404 | 945.63] loss=1.74 avg=2.27\n",
            "[405 | 947.85] loss=1.80 avg=2.27\n",
            "[406 | 950.08] loss=1.70 avg=2.26\n",
            "[407 | 952.32] loss=2.17 avg=2.26\n",
            "[408 | 954.54] loss=1.79 avg=2.26\n",
            "[409 | 956.76] loss=1.91 avg=2.25\n",
            "[410 | 958.99] loss=2.22 avg=2.25\n",
            "[411 | 961.22] loss=1.98 avg=2.25\n",
            "[412 | 963.45] loss=1.87 avg=2.25\n",
            "[413 | 965.68] loss=1.86 avg=2.24\n",
            "[414 | 967.91] loss=2.34 avg=2.24\n",
            "[415 | 970.14] loss=2.04 avg=2.24\n",
            "[416 | 972.37] loss=2.00 avg=2.24\n",
            "[417 | 974.60] loss=1.90 avg=2.24\n",
            "[418 | 976.83] loss=2.06 avg=2.23\n",
            "[419 | 979.06] loss=2.10 avg=2.23\n",
            "[420 | 981.29] loss=2.09 avg=2.23\n",
            "[421 | 983.52] loss=1.72 avg=2.23\n",
            "[422 | 985.75] loss=1.49 avg=2.22\n",
            "[423 | 987.97] loss=1.74 avg=2.21\n",
            "[424 | 990.20] loss=2.00 avg=2.21\n",
            "[425 | 992.43] loss=2.21 avg=2.21\n",
            "[426 | 994.66] loss=1.87 avg=2.21\n",
            "[427 | 996.88] loss=1.76 avg=2.20\n",
            "[428 | 999.11] loss=2.02 avg=2.20\n",
            "[429 | 1001.35] loss=2.28 avg=2.20\n",
            "[430 | 1003.57] loss=1.95 avg=2.20\n",
            "[431 | 1005.79] loss=1.97 avg=2.20\n",
            "[432 | 1008.01] loss=1.99 avg=2.20\n",
            "[433 | 1010.24] loss=1.84 avg=2.19\n",
            "[434 | 1012.46] loss=1.75 avg=2.19\n",
            "[435 | 1014.69] loss=2.09 avg=2.19\n",
            "[436 | 1016.92] loss=1.85 avg=2.18\n",
            "[437 | 1019.14] loss=1.71 avg=2.18\n",
            "[438 | 1021.37] loss=2.02 avg=2.18\n",
            "[439 | 1023.59] loss=1.94 avg=2.17\n",
            "[440 | 1025.82] loss=1.61 avg=2.17\n",
            "[441 | 1028.05] loss=2.04 avg=2.17\n",
            "[442 | 1030.27] loss=1.88 avg=2.16\n",
            "[443 | 1032.49] loss=2.08 avg=2.16\n",
            "[444 | 1034.72] loss=1.79 avg=2.16\n",
            "[445 | 1036.93] loss=1.83 avg=2.16\n",
            "[446 | 1039.17] loss=2.18 avg=2.16\n",
            "[447 | 1041.39] loss=1.87 avg=2.15\n",
            "[448 | 1043.61] loss=2.12 avg=2.15\n",
            "[449 | 1045.84] loss=1.74 avg=2.15\n",
            "[450 | 1048.07] loss=1.89 avg=2.15\n",
            "[451 | 1050.29] loss=1.91 avg=2.14\n",
            "[452 | 1052.52] loss=1.55 avg=2.14\n",
            "[453 | 1054.74] loss=2.15 avg=2.14\n",
            "[454 | 1056.96] loss=2.16 avg=2.14\n",
            "[455 | 1059.18] loss=1.93 avg=2.14\n",
            "[456 | 1061.41] loss=2.38 avg=2.14\n",
            "[457 | 1063.64] loss=1.97 avg=2.14\n",
            "[458 | 1065.86] loss=2.07 avg=2.14\n",
            "[459 | 1068.08] loss=1.72 avg=2.13\n",
            "[460 | 1070.31] loss=1.72 avg=2.13\n",
            "[461 | 1072.53] loss=2.05 avg=2.13\n",
            "[462 | 1074.76] loss=1.92 avg=2.13\n",
            "[463 | 1076.98] loss=1.71 avg=2.12\n",
            "[464 | 1079.21] loss=2.34 avg=2.12\n",
            "[465 | 1081.43] loss=2.32 avg=2.13\n",
            "[466 | 1083.66] loss=1.96 avg=2.12\n",
            "[467 | 1085.88] loss=2.14 avg=2.12\n",
            "[468 | 1088.11] loss=1.78 avg=2.12\n",
            "[469 | 1090.33] loss=2.04 avg=2.12\n",
            "[470 | 1092.56] loss=1.66 avg=2.11\n",
            "[471 | 1094.78] loss=1.97 avg=2.11\n",
            "[472 | 1097.00] loss=1.79 avg=2.11\n",
            "[473 | 1099.23] loss=1.40 avg=2.10\n",
            "[474 | 1101.46] loss=1.78 avg=2.10\n",
            "[475 | 1103.69] loss=1.99 avg=2.10\n",
            "[476 | 1105.91] loss=1.88 avg=2.10\n",
            "[477 | 1108.13] loss=1.71 avg=2.09\n",
            "[478 | 1110.36] loss=1.97 avg=2.09\n",
            "[479 | 1112.59] loss=1.71 avg=2.09\n",
            "[480 | 1114.82] loss=1.67 avg=2.08\n",
            "[481 | 1117.04] loss=1.64 avg=2.08\n",
            "[482 | 1119.27] loss=1.66 avg=2.07\n",
            "[483 | 1121.49] loss=1.73 avg=2.07\n",
            "[484 | 1123.72] loss=2.01 avg=2.07\n",
            "[485 | 1125.95] loss=1.75 avg=2.07\n",
            "[486 | 1128.17] loss=1.75 avg=2.06\n",
            "[487 | 1130.40] loss=1.63 avg=2.06\n",
            "[488 | 1132.61] loss=2.18 avg=2.06\n",
            "[489 | 1134.85] loss=2.18 avg=2.06\n",
            "[490 | 1137.08] loss=1.66 avg=2.06\n",
            "[491 | 1139.30] loss=1.80 avg=2.06\n",
            "[492 | 1141.53] loss=1.87 avg=2.05\n",
            "[493 | 1143.75] loss=1.56 avg=2.05\n",
            "[494 | 1145.97] loss=1.84 avg=2.05\n",
            "[495 | 1148.21] loss=1.61 avg=2.04\n",
            "[496 | 1150.43] loss=1.54 avg=2.04\n",
            "[497 | 1152.65] loss=1.61 avg=2.03\n",
            "[498 | 1154.88] loss=1.77 avg=2.03\n",
            "[499 | 1157.10] loss=2.00 avg=2.03\n",
            "[500 | 1159.32] loss=1.62 avg=2.03\n",
            "Saving checkpoint/run1/model-500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text generation"
      ],
      "metadata": {
        "id": "bUagiJzBTeoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"Is there a second Earth?\""
      ],
      "metadata": {
        "id": "qzTK7bdIPeOY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2.generate(sess, prefix=prefix, length=150)"
      ],
      "metadata": {
        "id": "ZCaaNXR7kI9I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb7be50c-5f68-4033-f8f4-60430a518f75"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is there a second Earth? Do you want to sleep?\" \n",
            "\"Not me,\" he said. \"I'm only a tree, I see it all around me.\" He stared up at the \n",
            "tree, at the faces of the men and women who had borne him with them, at the faces of the fires \n",
            "that had burned right overhead, at the faces of the horses that had not been able to come to a trot \n",
            "when the mules that surrounded them had gone down. He wanted to cry like a baby . . . but his tears \n",
            "were gone now. He wanted nothing better than to go home and live out his dream again. \n",
            "The words came unbidden to Bran's mouth. They were the only words he could\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Saving model to Google Drive (optional)"
      ],
      "metadata": {
        "id": "zlM6aQYZSccl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYXmOFl5Bjhv",
        "outputId": "564ebb74-2ba5-403f-dc4b-c36ce1478d52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='run1')"
      ],
      "metadata": {
        "id": "3RUjr4_ZluKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can find more texts e.g. on:\n",
        "https://www.gutenberg.org/cache/epub/1597/pg1597.txt\n",
        "</br></br>\n",
        "You can download them to Colab using code similar to the ones below."
      ],
      "metadata": {
        "id": "OUhaGg_uS6o5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://www.gutenberg.org/cache/epub/1597/pg1597.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7K9X3K8TEwj",
        "outputId": "d0760c42-a0e4-4dcf-b7cc-ca98aaffa2b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-21 14:49:16--  https://www.gutenberg.org/cache/epub/1597/pg1597.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 329071 (321K) [text/plain]\n",
            "Saving to: â€˜pg1597.txtâ€™\n",
            "\n",
            "pg1597.txt          100%[===================>] 321.36K   800KB/s    in 0.4s    \n",
            "\n",
            "2023-03-21 14:49:22 (800 KB/s) - â€˜pg1597.txtâ€™ saved [329071/329071]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://www.gutenberg.org/files/98/98-0.txt"
      ],
      "metadata": {
        "id": "HYL0wij2m4Gf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42bf360b-ce90-4a36-d434-44820124b877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-22 13:25:10--  https://www.gutenberg.org/files/98/98-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 807231 (788K) [text/plain]\n",
            "Saving to: â€˜98-0.txtâ€™\n",
            "\n",
            "98-0.txt            100%[===================>] 788.31K   718KB/s    in 1.1s    \n",
            "\n",
            "2023-02-22 13:25:12 (718 KB/s) - â€˜98-0.txtâ€™ saved [807231/807231]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://github.com/matt-dray/tng-stardate/tree/master/data/scripts"
      ],
      "metadata": {
        "id": "VClsbkgRxYvR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}